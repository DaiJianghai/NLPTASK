{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 无优化的简单版本 Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本预处理\n",
    "sentences = [\"jack like dog\", \"jack like cat\", \"jack like animal\",\n",
    "  \"dog cat animal\", \"banana apple cat dog like\", \"dog fish milk like\",\n",
    "  \"dog cat animal like\", \"jack like apple\", \"apple like\", \"jack like banana\",\n",
    "  \"apple banana jack movie book music like\", \"cat dog hate\", \"cat dog like\"]\n",
    "\n",
    "word_sequence = \" \".join(sentences).split() # ['jack', 'like', 'dog', 'jack', 'like', 'cat', 'animal',...]\n",
    "vocab = list(set(word_sequence)) # build words vocabulary\n",
    "word2idx = {w: i for i, w in enumerate(vocab)} # {'jack':0, 'like':1,...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Parameters\n",
    "batch_size = 8\n",
    "embedding_size = 2  # 2 dim vector represent one word\n",
    "C = 2 # window size\n",
    "voc_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "skip_grams = []\n",
    "for idx in range(C, len(word_sequence) - C):\n",
    "  center = word2idx[word_sequence[idx]] # center word\n",
    "  context_idx = list(range(idx - C, idx)) + list(range(idx + 1, idx + C + 1)) # context word idx\n",
    "  context = [word2idx[word_sequence[i]] for i in context_idx]\n",
    "  for w in context:\n",
    "    skip_grams.append([center, w])\n",
    "\n",
    "# 2.\n",
    "def make_data(skip_grams):\n",
    "  input_data = []\n",
    "  output_data = []\n",
    "  for i in range(len(skip_grams)):\n",
    "    input_data.append(np.eye(voc_size)[skip_grams[i][0]])\n",
    "    output_data.append(skip_grams[i][1])\n",
    "  return input_data, output_data\n",
    "\n",
    "# 3.\n",
    "input_data, output_data = make_data(skip_grams)\n",
    "input_data, output_data = torch.Tensor(input_data), torch.LongTensor(output_data)\n",
    "dataset = TensorDataset(input_data, output_data)\n",
    "loader = DataLoader(dataset, batch_size, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class Word2Vec(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Word2Vec, self).__init__()\n",
    "\n",
    "    # W and V is not Traspose relationship\n",
    "    self.W = nn.Parameter(torch.randn(voc_size, embedding_size))\n",
    "    self.V = nn.Parameter(torch.randn(embedding_size, voc_size))\n",
    "\n",
    "  def forward(self, X):\n",
    "    # X : [batch_size, voc_size] one-hot\n",
    "    # torch.mm only for 2 dim matrix, but torch.matmul can use to any dim\n",
    "    hidden_layer = torch.matmul(X, self.W) # hidden_layer : [batch_size, embedding_size]\n",
    "    output_layer = torch.matmul(hidden_layer, self.V) # output_layer : [batch_size, voc_size]\n",
    "    return output_layer\n",
    "\n",
    "model = Word2Vec().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "for epoch in range(2000):\n",
    "  for i, (batch_x, batch_y) in enumerate(loader):\n",
    "    batch_x = batch_x.to(device)\n",
    "    batch_y = batch_y.to(device)\n",
    "    pred = model(batch_x)\n",
    "    loss = criterion(pred, batch_y)\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "      print(epoch + 1, i, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "W, WT = model.parameters()\n",
    "for i, label in enumerate(vocab):\n",
    "  x,y = float(W[i][0]), float(W[i][1])\n",
    "  plt.scatter(x, y)\n",
    "  plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch实现 使用负采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "C = 3 # context window\n",
    "K = 15 # number of negative samples\n",
    "epochs = 2\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "EMBEDDING_SIZE = 100\n",
    "batch_size = 32\n",
    "lr = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "with open('./data/text8.train.txt') as f:\n",
    "    text = f.read() # 得到文本内容\n",
    "\n",
    "text = text.lower().split() #　分割成单词列表\n",
    "vocab_dict = dict(Counter(text).most_common(MAX_VOCAB_SIZE - 1)) # 得到单词字典表，key是单词，value是次数\n",
    "vocab_dict['<UNK>'] = len(text) - np.sum(list(vocab_dict.values())) # 把不常用的单词都编码为\"<UNK>\"， 此处市是 频次为1000的词\n",
    "word2idx = {word:i for i, word in enumerate(vocab_dict.keys())}\n",
    "idx2word = {i:word for i, word in enumerate(vocab_dict.keys())}\n",
    "word_counts = np.array([count for count in vocab_dict.values()], dtype=np.float32)\n",
    "word_freqs = word_counts / np.sum(word_counts)\n",
    "word_freqs = word_freqs ** (3./4.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造数据集 len 为文本单词数量 get_item需要获取中心词 和 背景词 以及负采样词\n",
    "class WordEmbeddingDataset(tud.Dataset):\n",
    "    def __init__(self, text, word2idx, word_freqs):\n",
    "        ''' text: a list of words, all text from the training dataset\n",
    "            word2idx: the dictionary from word to index\n",
    "            word_freqs: the frequency of each word\n",
    "        '''\n",
    "        super(WordEmbeddingDataset, self).__init__() # #通过父类初始化模型，然后重写两个方法\n",
    "        self.text_encoded = [word2idx.get(word, word2idx['<UNK>']) for word in text] # 把单词数字化表示。如果不在词典中，也表示为unk\n",
    "        self.text_encoded = torch.LongTensor(self.text_encoded) # nn.Embedding需要传入LongTensor类型\n",
    "        self.word2idx = word2idx\n",
    "        self.word_freqs = torch.Tensor(word_freqs)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_encoded) # 返回所有单词的总数，即item的总数\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ''' 这个function返回以下数据用于训练\n",
    "            - 中心词\n",
    "            - 这个单词附近的positive word\n",
    "            - 随机采样的K个单词作为negative word\n",
    "        '''\n",
    "        center_words = self.text_encoded[idx] # 取得中心词\n",
    "        pos_indices = list(range(idx - C, idx)) + list(range(idx + 1, idx + C + 1)) # 先取得中心左右各C个词的索引\n",
    "        pos_indices = [i % len(self.text_encoded) for i in pos_indices] # 为了避免索引越界，所以进行取余处理\n",
    "        pos_words = self.text_encoded[pos_indices] # tensor(list)\n",
    "        \n",
    "        neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0], True)\n",
    "        # torch.multinomial作用是对self.word_freqs做K * pos_words.shape[0]次取值，输出的是self.word_freqs对应的下标\n",
    "        # 取样方式采用有放回的采样，并且self.word_freqs数值越大，取样概率越大\n",
    "        # 每采样一个正确的单词(positive word)，就采样K个错误的单词(negative word)，pos_words.shape[0]是正确单词数量\n",
    "        \n",
    "        # while 循环是为了保证 neg_words中不能包含背景词\n",
    "        while len(set(pos_indices) & set(neg_words.numpy().tolist())) > 0:\n",
    "            neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0], True)\n",
    "\n",
    "        return center_words, pos_words, neg_words\n",
    "dataset = WordEmbeddingDataset(text, word2idx, word_freqs)\n",
    "dataloader = tud.DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4813),\n",
       " tensor([  50, 9999,  393, 3139,   11,    5]),\n",
       " tensor([ 899,    6, 6510,   46,  224, 9998, 4840, 1604,  671, 3631, 9999,    6,\n",
       "         3748, 4352,  264, 1127,  348, 8488,   38, 3089, 2315, 1026,   57, 2959,\n",
       "          314, 5618, 2434,  120, 9999,   38,   37, 4345,  311, 1039, 7227, 3657,\n",
       "           13, 3005, 9999,  191, 1944, 1705, 7465,  665, 2772, 4282,   63,  351,\n",
       "         6303, 2229, 6091,   10, 7591, 3934, 4744, 1582, 7354, 3633, 1149,  511,\n",
       "           14,  126, 4361, 7679, 8641,    5,  146,  222, 9846, 5108,    8,  413,\n",
       "         1986, 5962, 1580, 9999, 2053,   14,   42,   91,  603,  154, 2729, 9117,\n",
       "           52,  964, 9999,  286,   50,  888]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size) # 作为中心词的embedding\n",
    "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size) # 作为背景词的embedding\n",
    "        \n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        ''' input_labels: center words, [batch_size]\n",
    "            pos_labels: positive words, [batch_size, (window_size * 2)]\n",
    "            neg_labels：negative words, [batch_size, (window_size * 2 * K)]\n",
    "            \n",
    "            return: loss, [batch_size]\n",
    "        '''\n",
    "        input_embedding = self.in_embed(input_labels) # [batch_size, embed_size]\n",
    "        pos_embedding = self.out_embed(pos_labels)# [batch_size, (window * 2), embed_size]\n",
    "        neg_embedding = self.out_embed(neg_labels) # [batch_size, (window * 2 * K), embed_size]\n",
    "        \n",
    "        input_embedding = input_embedding.unsqueeze(2) # [batch_size, embed_size, 1]\n",
    "        \n",
    "        pos_dot = torch.bmm(pos_embedding, input_embedding) # [batch_size, (window * 2), 1], 这一步要求和背景词接近\n",
    "        pos_dot = pos_dot.squeeze(2) # [batch_size, (window * 2)]\n",
    "        \n",
    "        neg_dot = torch.bmm(neg_embedding, -input_embedding) # [batch_size, (window * 2 * K), 1]，这一步取负号要求和负采样词远离\n",
    "        neg_dot = neg_dot.squeeze(2) # batch_size, (window * 2 * K)]\n",
    "        \n",
    "        log_pos = F.logsigmoid(pos_dot).sum(1) # .sum()结果只为一个数，.sum(1)结果是一维的张量\n",
    "        log_neg = F.logsigmoid(neg_dot).sum(1)\n",
    "        \n",
    "        loss = log_pos + log_neg  # 理解成似然 = log(positive) + log(-negative)\n",
    "        \n",
    "        return -loss # 最小化\n",
    "    \n",
    "    def input_embedding(self):\n",
    "        return self.in_embed.weight.detach().cpu().numpy()\n",
    "\n",
    "model = EmbeddingModel(MAX_VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3304/1683031634.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m#     print('epoch', e, 'iteration', i, loss.item())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0membedding_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"embedding-{}.pt\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEMBEDDING_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3304/3004087855.py\u001b[0m in \u001b[0;36minput_embedding\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minput_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_embed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEmbeddingModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_VOCAB_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEMBEDDING_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "for e in range(1):\n",
    "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "        input_labels = input_labels.long().to(device)\n",
    "        pos_labels = pos_labels.long().to(device)\n",
    "        neg_labels = neg_labels.long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_labels, pos_labels, neg_labels).mean()\n",
    "        loss.backward() \n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        writer.add_scalar('Loss/train', loss.item(), i)\n",
    "        # if i % 100 == 0:\n",
    "        #     print('epoch', e, 'iteration', i, loss.item())\n",
    "\n",
    "embedding_weights = model.input_embedding()\n",
    "torch.save(model.state_dict(), \"embedding-{}.pt\".format(EMBEDDING_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 寻找相似的单词\n",
    "import scipy\n",
    "def find_nearest(word):\n",
    "    index = word2idx[word]\n",
    "    embedding = embedding_weights[index]\n",
    "    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
    "    return [idx2word[i] for i in cos_dis.argsort()[:10]]\n",
    "for word in [\"two\", \"america\", \"computer\"]:\n",
    "    print(word, find_nearest(word))\n",
    "# 输出\n",
    "# two ['two', 'zero', 'four', 'one', 'six', 'five', 'three', 'nine', 'eight', 'seven']\n",
    "# america ['america', 'states', 'japan', 'china', 'usa', 'west', 'africa', 'italy', 'united', 'kingdom']\n",
    "# computer ['computer', 'machine', 'earth', 'pc', 'game', 'writing', 'board', 'result', 'code', 'website']"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12648cd03e9b8cb938ff511672062539752d7c15a7521866f92e1f90f92eb1a9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('work')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
